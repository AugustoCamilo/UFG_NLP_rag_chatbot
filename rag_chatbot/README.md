# Configura√ß√£o do Ambiente Anaconda

## 1. Cria√ß√£o do Ambiente Conda

Primeiro, vamos criar um ambiente Conda dedicado para isolar o projeto.

Abra o terminal **Anaconda Prompt** e execute os seguintes comandos:

```bash
conda create -n rag_solution python=3.10 -y
conda activate rag_solution
```

## 2. Instala√ß√£o das Depend√™ncias

Instale os pacotes necess√°rios usando `pip`. A biblioteca `llama-cpp-python` pode exigir etapas de compila√ß√£o, mas geralmente funciona bem via pip na maioria dos sistemas.

### Instala√ß√£o principal:

```bash
pip install langchain langchain-community flask chromadb sentence-transformers pypdf
```

### Para o LLM local:

Isso compilar√° o modelo para sua CPU (ou GPU, se configurado). Pode demorar alguns minutos.

```bash
pip install llama-cpp-python
```

### Instala√ß√£o para uso da barra de progress√£o:

```bash
pip install tqdm
```

---

# Download do Modelo de Linguagem (LLM)

Para uma solu√ß√£o open-source e local, usaremos um modelo no formato **GGUF**.

## 1. Acesso ao Hugging Face

Visite o reposit√≥rio no Hugging Face, por exemplo:

üîó [TheBloke/Mistral-7B-Instruct-v0.2-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)

## 2. Escolha do Arquivo

Baixe um arquivo GGUF **quantizado**. Recomenda-se um modelo de tamanho m√©dio, como:

```
mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

## 3. Salvamento do Arquivo

Ap√≥s o download, salve o arquivo dentro da pasta:

```
models/
```
